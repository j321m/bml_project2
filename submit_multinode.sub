#!/bin/bash
#
#SBATCH --job-name=bml_torchrun
#SBATCH --partition=plgrid-gpu-a100
#SBATCH --account=plgllmparamgr-gpu-a100  # Account name
#SBATCH --nodes=2
#SBATCH --ntasks=2
#SBATCH --gpus-per-task=1
#SBATCH --cpus-per-gpu=8            # Number of CPUs per GPU
#SBATCH --mem=60G                  # Memory requested
#SBATCH --time=0:20:00
#SBATCH --output=logs/output_multinode_test_%j.txt

echo SLURM_NNODES=$SLURM_NNODES
echo RANDOM=$RANDOM
echo head_node_ip=$head_node_ip
echo SLURM_JOB_NODELIST=$SLURM_JOB_NODELIST

nodes=( $( scontrol show hostnames $SLURM_JOB_NODELIST ) )
echo nodes=$nodes
nodes_array=($nodes)
head_node=${nodes_array[0]}
echo $head_node
head_node_ip=$(srun --nodes=1 --ntasks=1 -w "$head_node" hostname --ip-address)
echo $head_node_ip


srun torchrun \
    --nnodes 2 \
    --nproc_per_node 1 \
    --rdzv_id $RANDOM \
    --rdzv_backend c10d \
    --rdzv_endpoint $head_node_ip:29500  \
  main.py